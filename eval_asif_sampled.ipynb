{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec214237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss \n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from asif import extract_candidate_sets_from_clusters\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from operator import itemgetter \n",
    "from typing import Tuple, List, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7658bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_represent(y: torch.Tensor, basis: torch.Tensor, non_zeros: int = 800, max_gpu_mem_gb: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute the sparse decomposition of a tensor y with respect to a basis, \n",
    "    considering the available GPU memory.\n",
    "    \n",
    "    Args:\n",
    "        y (torch.Tensor): Vectors to represent.\n",
    "        basis (torch.Tensor): Basis to represent with respect to.\n",
    "        non_zeros (int): Nonzero entries in the relative representation.\n",
    "        max_gpu_mem_gb (int): Maximum GPU memory allowed to use in gigabytes.\n",
    "        \n",
    "    Returns:\n",
    "        indices (torch.Tensor): Indices of the nonzero entries in each relative representation of y.\n",
    "        values (torch.Tensor): Corresponding coefficients of the entries.\n",
    "    \"\"\"\n",
    "    values, indices = torch.zeros((y.shape[0], non_zeros)), torch.zeros((y.shape[0], non_zeros), dtype=torch.long)\n",
    "\n",
    "    free_gpu_mem = max_gpu_mem_gb * 1024 ** 3\n",
    "    max_floats_in_mem = free_gpu_mem / 4\n",
    "    max_chunk_y = max_floats_in_mem / basis.shape[0]\n",
    "    n_chunks = int(y.shape[0] / max_chunk_y) + 1  \n",
    "    chunk_y = int(y.shape[0] / n_chunks) + n_chunks\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for c in range(n_chunks):\n",
    "            in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "            values[c * chunk_y : (c + 1) * chunk_y], indices[c * chunk_y : (c + 1) * chunk_y] = torch.topk(in_prods, non_zeros, dim=1)\n",
    "            del in_prods\n",
    "\n",
    "    return indices.to('cpu'), values.to('cpu')\n",
    "\n",
    "def relative_represent_2(y: torch.Tensor, basis: torch.Tensor, batch_size: int = 100,  k: int = 800, basis_batch_size: int = -1, computing_device: str = \"cpu\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    values, indices = torch.zeros((y.shape[0], k)), torch.zeros((y.shape[0], k), dtype=torch.long)\n",
    "\n",
    "    if basis_batch_size > 0:\n",
    "        y = y.to(\"cpu\")\n",
    "        basis = basis.to(\"cpu\")\n",
    "        for basis_i in range(0, y.size()[0], basis_batch_size):\n",
    "            basis_batch = basis[basis_i: min(basis_i+basis_batch_size, basis.size()[0])].to(computing_device)\n",
    "            for i in tqdm(range(0, y.size()[0], batch_size), disable=batch_size==y.size()[0]):\n",
    "                #in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "                yy = y[i: min(i+batch_size, y.size()[0])].to(computing_device)\n",
    "                sim = (1 / (1 + torch.cdist(yy, basis_batch))) \n",
    "                top_k = torch.topk(sim, k, dim=1)\n",
    "                values[i: min(i+batch_size, y.size()[0])], indices[i: min(i+batch_size, y.size()[0])] = top_k[0], top_k[1] + basis_i\n",
    "                del sim\n",
    "                del yy\n",
    "                torch.cuda.empty_cache()\n",
    "            del basis_batch\n",
    "\n",
    "    else:\n",
    "        for i in tqdm(range(0, y.size()[0], batch_size), disable=batch_size==y.size()[0]):\n",
    "            #in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "            sim = (1 / (1 + torch.cdist( y[i: min(i+batch_size, y.size()[0])], basis))) \n",
    "            values[i: min(i+batch_size, y.size()[0])], indices[i: min(i+batch_size, y.size()[0])] = torch.topk(sim, k, dim=1)\n",
    "            del sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return indices.to('cpu'), values.to('cpu')\n",
    "\n",
    "def sparsify(i: torch.Tensor, v: torch.Tensor, size: torch.Size) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Organize indices and values of n vectors into a single sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        i (torch.Tensor): indices of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        v (torch.Tensor): values of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        size (torch.Size): shape of the output tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: sparse tensor of shape \"size\" (n_vectors, zero + nonzero elements)\n",
    "    \"\"\"\n",
    "    flat_dim = len(i.flatten())\n",
    "    coo_first_row_idxs = torch.div(torch.arange(flat_dim), i.shape[1], rounding_mode='floor')\n",
    "    stacked_idxs = torch.cat((coo_first_row_idxs.unsqueeze(0), i.flatten().unsqueeze(0)), 0)\n",
    "    return torch.sparse_coo_tensor(stacked_idxs, v.flatten(), size)\n",
    "\n",
    "\n",
    "def normalize_sparse(tensor: torch.sparse.FloatTensor, nnz_per_row: int) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Normalize a sparse tensor by row.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.sparse.FloatTensor): The sparse tensor to normalize.\n",
    "        nnz_per_row (int): The number of non-zero elements per row.\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: The normalized sparse tensor.\n",
    "    \"\"\"\n",
    "    norms = torch.sparse.sum(tensor * tensor, dim=1).to_dense()\n",
    "    v = tensor._values().clone().detach().reshape(-1, nnz_per_row).t()\n",
    "    v /= torch.sqrt(norms)\n",
    "    return torch.sparse_coo_tensor(tensor._indices(), v.t().flatten(), tensor.shape)\n",
    "\n",
    "\n",
    "def standardize(tensor):\n",
    "    means = tensor.mean(dim=1, keepdim=True)\n",
    "    stds = tensor.std(dim=1, keepdim=True)\n",
    "    return (tensor - means) / stds\n",
    "\n",
    "def compute_relative_coordinates(embeddings, anchors, k, p=7):\n",
    "\n",
    "    embeddings = standardize(embeddings)\n",
    "    anchors = standardize(anchors)\n",
    "                \n",
    "    sim = (1 / (1 + torch.cdist(embeddings, anchors)))\n",
    "    \n",
    "    result = torch.zeros(sim.size())\n",
    "        \n",
    "    for i, j in enumerate(torch.argsort(sim, descending=True)[:,:k]):\n",
    "        result[i][j] = p\n",
    "    \n",
    "    return torch.nn.functional.normalize(result)\n",
    "\n",
    "def compute_self_relative_coordinates(embeddings, anchors, batch_size=1_000, denoise=True, k=800, p=8, device=\"cpu\"):\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(range(0, embeddings.size()[0], batch_size), disable=batch_size==embeddings.size()[0]):\n",
    "        self_relative_coordinates_batch = compute_relative_coordinates(embeddings[i:min(i+batch_size, embeddings.size()[0])], embeddings)\n",
    "        self_relative_coordinates_batch = self_relative_coordinates_batch.to(device)\n",
    "        relative_coordinates_vs_anchors = compute_relative_coordinates(self_relative_coordinates_batch, anchors, denoise=denoise, k=k, p=p)\n",
    "        relative_coordinates_vs_anchors = relative_coordinates_vs_anchors.to(\"cpu\")\n",
    "        result.append(relative_coordinates_vs_anchors)\n",
    "    \n",
    "    return torch.vstack(result)\n",
    "\n",
    "def evaluate_asif(relative_coordinates_1, relative_coordinates_2, n_of_samples):\n",
    "    distances_1_to_2 = 1 / (1 + torch.cdist(relative_coordinates_1, relative_coordinates_2))\n",
    "\n",
    "    # Get the max similarity for each vecotry in relative_coordinates_1\n",
    "    max_values = distances_1_to_2.max(dim=1, keepdim=True)\n",
    "    correct = 0\n",
    "\n",
    "    for sample_index in range(n_of_samples):\n",
    "        \n",
    "        # Get indexes of elements with max similarity\n",
    "        indexes = (distances_1_to_2[sample_index] == max_values.values[sample_index]).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "\n",
    "        # Check if the index of the current element is among the elements with the maximum similarity\n",
    "        if indexes.__contains__(sample_index):\n",
    "            correct = correct + 1\n",
    "\n",
    "        #retrieved_elements = itemgetter(*indexes)(elements_2)\n",
    "        #if elements_2[sample_index] in retrieved_elements:\n",
    "        #    correct = correct + 1\n",
    "    \n",
    "    return correct, int((correct/n_of_samples)*100)\n",
    "\n",
    "\n",
    "def elbow(X, cluster_sizes, label = \"Elbow curve\"):\n",
    "    distorsions = []\n",
    "    result = {}\n",
    "    for k in tqdm(cluster_sizes):\n",
    "        kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        distorsions.append(kmeans.inertia_)\n",
    "        result[k] = kmeans\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(cluster_sizes, distorsions)\n",
    "    plt.xticks(cluster_sizes)\n",
    "    plt.grid(True)\n",
    "    plt.title(label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1ec187",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "computing_device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74528a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "lyrics = pickle.load(open(f\"{data_folder}/lyrics.pkl\", \"rb\"))\n",
    "chords = pickle.load(open(f\"{data_folder}/chords.pkl\", \"rb\"))\n",
    "artist_song = pickle.load(open(f\"{data_folder}/artist_song.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "lyrics_embeddings = pickle.load(open(f\"{data_folder}/lyrics_embeddings_sbert_roberta.pkl\", \"rb\"))\n",
    "chords_embeddings = pickle.load(open(f\"{data_folder}/chords_embeddings_sbert_chocolm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909499db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_samples = 10_000\n",
    "\n",
    "sample = np.random.choice(np.intersect1d(np.unique(chords, return_index=True)[1], np.unique(lyrics, return_index=True)[1]), n_of_samples)\n",
    "\n",
    "chords_embeddings_sampled = chords_embeddings[sample]\n",
    "lyrics_embeddings_sampled = lyrics_embeddings[sample]\n",
    "\n",
    "lyrics_sampled = itemgetter(*sample)(lyrics)\n",
    "chords_sampled = itemgetter(*sample)(chords)\n",
    "\n",
    "computing_device = \"cuda:0\"\n",
    "\n",
    "chords_embeddings_sampled = chords_embeddings_sampled.to(torch.float32).to(computing_device)\n",
    "lyrics_embeddings_sampled = lyrics_embeddings_sampled.to(torch.float32).to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4a9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10000 100 9986 99\n",
      "2 9995 99 9995 99\n",
      "3 9798 97 9761 97\n",
      "4 9543 95 9541 95\n",
      "5 9253 92 9245 92\n",
      "6 8825 88 8864 88\n",
      "7 8408 84 8430 84\n",
      "8 7824 78 7818 78\n",
      "9 7188 71 7153 71\n",
      "16 2175 21 1913 19\n",
      "32 388 3 343 3\n",
      "64 66 0 64 0\n",
      "128 23 0 17 0\n"
     ]
    }
   ],
   "source": [
    "# test chords to lyrics\n",
    "\n",
    "ks = [p for p in range(1, 10)] + [2**p for p in [4, 5, 6, 7]]\n",
    "\n",
    "for k in ks:\n",
    "    relative_coordinates_chords = compute_relative_coordinates(chords_embeddings_sampled, chords_embeddings_sampled, k=k)\n",
    "    relative_coordinates_lyrics = compute_relative_coordinates(lyrics_embeddings_sampled, lyrics_embeddings_sampled, k=k)\n",
    "    correct_chords_to_lyrics, accuracy_chord_to_lyrics =  evaluate_asif(relative_coordinates_chords, relative_coordinates_lyrics, n_of_samples)\n",
    "    correct_lyrics_to_chords, accuracy_lyrics_to_chords = evaluate_asif(relative_coordinates_lyrics, relative_coordinates_chords, n_of_samples)\n",
    "    print(k, correct_chords_to_lyrics, accuracy_chord_to_lyrics, correct_lyrics_to_chords, accuracy_lyrics_to_chords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
