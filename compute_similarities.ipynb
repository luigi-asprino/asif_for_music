{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss \n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from asif import extract_candidate_sets_from_clusters\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from operator import itemgetter \n",
    "from typing import Tuple, List, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_represent(y: torch.Tensor, basis: torch.Tensor, non_zeros: int = 800, max_gpu_mem_gb: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute the sparse decomposition of a tensor y with respect to a basis, \n",
    "    considering the available GPU memory.\n",
    "    \n",
    "    Args:\n",
    "        y (torch.Tensor): Vectors to represent.\n",
    "        basis (torch.Tensor): Basis to represent with respect to.\n",
    "        non_zeros (int): Nonzero entries in the relative representation.\n",
    "        max_gpu_mem_gb (int): Maximum GPU memory allowed to use in gigabytes.\n",
    "        \n",
    "    Returns:\n",
    "        indices (torch.Tensor): Indices of the nonzero entries in each relative representation of y.\n",
    "        values (torch.Tensor): Corresponding coefficients of the entries.\n",
    "    \"\"\"\n",
    "    values, indices = torch.zeros((y.shape[0], non_zeros)), torch.zeros((y.shape[0], non_zeros), dtype=torch.long)\n",
    "\n",
    "    free_gpu_mem = max_gpu_mem_gb * 1024 ** 3\n",
    "    max_floats_in_mem = free_gpu_mem / 4\n",
    "    max_chunk_y = max_floats_in_mem / basis.shape[0]\n",
    "    n_chunks = int(y.shape[0] / max_chunk_y) + 1  \n",
    "    chunk_y = int(y.shape[0] / n_chunks) + n_chunks\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for c in range(n_chunks):\n",
    "            in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "            values[c * chunk_y : (c + 1) * chunk_y], indices[c * chunk_y : (c + 1) * chunk_y] = torch.topk(in_prods, non_zeros, dim=1)\n",
    "            del in_prods\n",
    "\n",
    "    return indices.to('cpu'), values.to('cpu')\n",
    "\n",
    "def relative_represent_2(y: torch.Tensor, basis: torch.Tensor, batch_size: int = 100,  k: int = 800, basis_batch_size: int = -1, computing_device: str = \"cpu\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    values, indices = torch.zeros((y.shape[0], k)), torch.zeros((y.shape[0], k), dtype=torch.long)\n",
    "\n",
    "    if basis_batch_size > 0:\n",
    "        y = y.to(\"cpu\")\n",
    "        basis = basis.to(\"cpu\")\n",
    "        for basis_i in range(0, y.size()[0], basis_batch_size):\n",
    "            basis_batch = basis[basis_i: min(basis_i+basis_batch_size, basis.size()[0])].to(computing_device)\n",
    "            for i in tqdm(range(0, y.size()[0], batch_size), disable=batch_size==y.size()[0]):\n",
    "                #in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "                yy = y[i: min(i+batch_size, y.size()[0])].to(computing_device)\n",
    "                sim = (1 / (1 + torch.cdist(yy, basis_batch))) \n",
    "                top_k = torch.topk(sim, k, dim=1)\n",
    "                values[i: min(i+batch_size, y.size()[0])], indices[i: min(i+batch_size, y.size()[0])] = top_k[0], top_k[1] + basis_i\n",
    "                del sim\n",
    "                del yy\n",
    "                torch.cuda.empty_cache()\n",
    "            del basis_batch\n",
    "\n",
    "    else:\n",
    "        for i in tqdm(range(0, y.size()[0], batch_size), disable=batch_size==y.size()[0]):\n",
    "            #in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "            sim = (1 / (1 + torch.cdist( y[i: min(i+batch_size, y.size()[0])], basis))) \n",
    "            values[i: min(i+batch_size, y.size()[0])], indices[i: min(i+batch_size, y.size()[0])] = torch.topk(sim, k, dim=1)\n",
    "            del sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return indices.to('cpu'), values.to('cpu')\n",
    "\n",
    "def sparsify(i: torch.Tensor, v: torch.Tensor, size: torch.Size) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Organize indices and values of n vectors into a single sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        i (torch.Tensor): indices of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        v (torch.Tensor): values of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        size (torch.Size): shape of the output tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: sparse tensor of shape \"size\" (n_vectors, zero + nonzero elements)\n",
    "    \"\"\"\n",
    "    flat_dim = len(i.flatten())\n",
    "    coo_first_row_idxs = torch.div(torch.arange(flat_dim), i.shape[1], rounding_mode='floor')\n",
    "    stacked_idxs = torch.cat((coo_first_row_idxs.unsqueeze(0), i.flatten().unsqueeze(0)), 0)\n",
    "    return torch.sparse_coo_tensor(stacked_idxs, v.flatten(), size)\n",
    "\n",
    "\n",
    "def normalize_sparse(tensor: torch.sparse.FloatTensor, nnz_per_row: int) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Normalize a sparse tensor by row.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.sparse.FloatTensor): The sparse tensor to normalize.\n",
    "        nnz_per_row (int): The number of non-zero elements per row.\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: The normalized sparse tensor.\n",
    "    \"\"\"\n",
    "    norms = torch.sparse.sum(tensor * tensor, dim=1).to_dense()\n",
    "    v = tensor._values().clone().detach().reshape(-1, nnz_per_row).t()\n",
    "    v /= torch.sqrt(norms)\n",
    "    return torch.sparse_coo_tensor(tensor._indices(), v.t().flatten(), tensor.shape)\n",
    "\n",
    "\n",
    "def standardize(tensor):\n",
    "    means = tensor.mean(dim=1, keepdim=True)\n",
    "    stds = tensor.std(dim=1, keepdim=True)\n",
    "    return (tensor - means) / stds\n",
    "\n",
    "def compute_relative_coordinates(embeddings, anchors, k, p=7):\n",
    "\n",
    "    embeddings = standardize(embeddings)\n",
    "    anchors = standardize(anchors)\n",
    "                \n",
    "    sim = (1 / (1 + torch.cdist(embeddings, anchors)))\n",
    "    \n",
    "    #result = torch.zeros(sim.size())\n",
    "        \n",
    "    #for i, j in enumerate(torch.argsort(sim, descending=True)[:,:k]):\n",
    "    #    result[i][j] = p\n",
    "    indices = [[i, int(j)] for i,j in enumerate(torch.argsort(sim, descending=True)[:,:k])]\n",
    "    values = [p] * len(indices)\n",
    "    print(indices)\n",
    "    print(values)\n",
    "    print(sim.size())\n",
    "    \n",
    "    return torch.sparse_coo_tensor(indices=indices, values=values, size=sim.size())\n",
    "\n",
    "def compute_self_relative_coordinates(embeddings, anchors, batch_size=1_000, denoise=True, k=800, p=8, device=\"cpu\"):\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(range(0, embeddings.size()[0], batch_size), disable=batch_size==embeddings.size()[0]):\n",
    "        self_relative_coordinates_batch = compute_relative_coordinates(embeddings[i:min(i+batch_size, embeddings.size()[0])], embeddings)\n",
    "        self_relative_coordinates_batch = self_relative_coordinates_batch.to(device)\n",
    "        relative_coordinates_vs_anchors = compute_relative_coordinates(self_relative_coordinates_batch, anchors, denoise=denoise, k=k, p=p)\n",
    "        relative_coordinates_vs_anchors = relative_coordinates_vs_anchors.to(\"cpu\")\n",
    "        result.append(relative_coordinates_vs_anchors)\n",
    "    \n",
    "    return torch.vstack(result)\n",
    "\n",
    "def evaluate_asif(relative_coordinates_1, relative_coordinates_2, n_of_samples):\n",
    "    distances_1_to_2 = 1 / (1 + torch.cdist(relative_coordinates_1, relative_coordinates_2))\n",
    "\n",
    "    # Get the max similarity for each vecotry in relative_coordinates_1\n",
    "    max_values = distances_1_to_2.max(dim=1, keepdim=True)\n",
    "    correct = 0\n",
    "\n",
    "    for sample_index in range(n_of_samples):\n",
    "        \n",
    "        # Get indexes of elements with max similarity\n",
    "        indexes = (distances_1_to_2[sample_index] == max_values.values[sample_index]).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "\n",
    "        # Check if the index of the current element is among the elements with the maximum similarity\n",
    "        if indexes.__contains__(sample_index):\n",
    "            correct = correct + 1\n",
    "\n",
    "        #retrieved_elements = itemgetter(*indexes)(elements_2)\n",
    "        #if elements_2[sample_index] in retrieved_elements:\n",
    "        #    correct = correct + 1\n",
    "    \n",
    "    return correct, int((correct/n_of_samples)*100)\n",
    "\n",
    "\n",
    "def elbow(X, cluster_sizes, label = \"Elbow curve\"):\n",
    "    distorsions = []\n",
    "    result = {}\n",
    "    for k in tqdm(cluster_sizes):\n",
    "        kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        distorsions.append(kmeans.inertia_)\n",
    "        result[k] = kmeans\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(cluster_sizes, distorsions)\n",
    "    plt.xticks(cluster_sizes)\n",
    "    plt.grid(True)\n",
    "    plt.title(label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "computing_device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3518/3518 [37:59<00:00,  1.54it/s]\n",
      "100%|██████████| 3518/3518 [37:58<00:00,  1.54it/s]\n",
      "100%|██████████| 3518/3518 [37:58<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "lyrics_embeddings = pickle.load(open(f\"{data_folder}/lyrics_embeddings_sbert_roberta.pkl\", \"rb\"))\n",
    "il, vl = relative_represent_2(lyrics_embeddings, lyrics_embeddings, computing_device=computing_device, batch_size=1000, basis_batch_size=int(lyrics_embeddings.size()[0]/3) + 1)\n",
    "pickle.dump(il, open(f\"{data_folder}/lyrics_indexes_similarities.pkl\", \"wb\"))\n",
    "pickle.dump(vl, open(f\"{data_folder}/lyrics_values_similarities.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7035/7035 [23:43<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Computing chord embeddings\n",
    "chords_embeddings = pickle.load(open(f\"{data_folder}/chords_embeddings_sbert_chocolm.pkl\", \"rb\"))\n",
    "chord_embeddings_gpu = chords_embeddings.to(computing_device)\n",
    "del chords_embeddings\n",
    "i, v = relative_represent(chord_embeddings_gpu, chord_embeddings_gpu, 500)\n",
    "del chord_embeddings_gpu\n",
    "pickle.dump(i, open(f\"{data_folder}/chords_indexes_similarities.pkl\", \"wb\"))\n",
    "pickle.dump(v, open(f\"{data_folder}/chords_values_similarities.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
